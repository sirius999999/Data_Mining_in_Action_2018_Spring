{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "import pymystem3\n",
    "mystem = pymystem3.Mystem()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно классифировать фамили:\n",
    "- 0 если не фамилия\n",
    "- 1 если фамилия\n",
    "Классы несбалансированы нужно использовать roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(index = train[train.Word.duplicated()].index, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer поддерживает подсчет N-грам слов или последовательностей символов. Векторизатор строит словарь индексов признаков\n",
    "Значение индекса слова в словаре связано с его частотой употребления во всем обучающем корпусе.\n",
    "Будем использовать Наивный Байесовский классификатор и scikit-learn включает в себя несколько вариантов этого классификатора. Самый подходящий для подсчета слов — это его поли номинальный вариант.\n",
    "\n",
    "Создадим конвеер для того, чтобы было продбирать гиперпараметры с помощью GridSearchCV, не забывая при этом разбивать на фолды. В качестве теста пробую Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f540b79bf98>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54581b7e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = list(u'йцкнгшщзхъфвпрлджчсмтьб'+u'йцкнгшщзхъфвпрлджчсмтьб'.upper())\n",
    "vowels = list(u'уеёыаоэяию'+u'уеёыаоэяию'.upper())\n",
    "\n",
    "def name_from_pymystem(word):\n",
    "    try:\n",
    "        return 1 if 'имя' in mystem.analyze(word)[0]['analysis'][0]['gr'].split(',') else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def surn_from_pymystem(word):\n",
    "    try:\n",
    "        return 1 if 'фам' in mystem.analyze(word)[0]['analysis'][0]['gr'].split(',') else 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def up_cases(word):\n",
    "    return sum(1 for c in word if c.isupper())\n",
    "\n",
    "def low_cases(word):\n",
    "    return sum(1 for c in word if c.isupper())\n",
    "\n",
    "def vow(word):\n",
    "    return sum(1 for c in word if c in vowels)\n",
    "\n",
    "def con(word):\n",
    "    return sum(1 for c in word if c in consonants)\n",
    "\n",
    "def spec(word):\n",
    "    return sum(1 for c in word if c not in consonants or c not in vowels)\n",
    "\n",
    "def len_eq(word):\n",
    "    if len(word) == con(word) or len(word) == vow(word) or len(word) < 4 or len(word) > 13:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Make_x(X_train):\n",
    "    X_train['name_from_pymystem'] = X_train['Word'].apply(name_from_pymystem)\n",
    "    X_train['surn_from_pymystem'] = X_train['Word'].apply(surn_from_pymystem)\n",
    "    X_train['up_cases'] = X_train['Word'].apply(up_cases)\n",
    "    X_train['low_cases'] = X_train['Word'].apply(low_cases)\n",
    "    X_train['con'] = X_train['Word'].apply(con)\n",
    "    X_train['vow'] = X_train['Word'].apply(vow)\n",
    "    X_train['len'] = X_train['Word'].apply(lambda word: len(list(word)))\n",
    "    \n",
    "    \n",
    "    return X_train[['name_from_pymystem','surn_from_pymystem',\n",
    "                    'up_cases', 'low_cases',\n",
    "                    'con', 'vow','len']].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(index = train[((train['Word'].apply(len_eq) == 1) | (train['Word'].apply(spec) <= 1) )\n",
    "                                 & (train['Label'] == 1)].index, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_grid_reg(X_train, Y_train, c=[10.0 ** i for i in range(-3, 5)]):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cc = CountVectorizer(encoding = 'unicode',\n",
    "                         lowercase = False,\n",
    "                         analyzer = 'char', \n",
    "                         ngram_range = (2,7), \n",
    "                         min_df = 3, \n",
    "                         max_df = 14)\n",
    "    X_train_cc = cc.fit_transform(X_train['Word'].as_matrix())\n",
    "    \n",
    "    X_train = hstack((X_train_cc, Make_x(X_train)))\n",
    "    \n",
    "    clf = LogisticRegression(class_weight = 'balanced', solver = 'sag', n_jobs=1)\n",
    "    \n",
    "    grid=  {'C': c,\n",
    "            'penalty' : ['l2'],\n",
    "            'max_iter' : [10000]\n",
    "           }\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    gs = GridSearchCV(clf, scoring='roc_auc', \n",
    "                      param_grid=grid, cv=cv, \n",
    "                      return_train_score=True, n_jobs=-1, verbose = True)\n",
    "    gs.fit(X_train,Y_train)\n",
    "    \n",
    "    print ('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print (max(gs.cv_results_['mean_test_score']))\n",
    "    \n",
    "    return gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_grid_lgb(X_train, Y_train):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cc = CountVectorizer(encoding = 'unicode',\n",
    "                         lowercase = False,\n",
    "                         analyzer = 'char', \n",
    "                         ngram_range = (2,7), \n",
    "                         min_df = 3, \n",
    "                         max_df = 14,\n",
    "                         dtype= np.float32)\n",
    "    X_train_cc = cc.fit_transform(X_train['Word'].as_matrix())\n",
    "    \n",
    "    X_train = hstack((X_train_cc, Make_x(X_train)))\n",
    "    \n",
    "    # Create parameters to search\n",
    "    gridParams = {\n",
    "        'n_estimators': [90],\n",
    "        'learning_rate': [0.7],\n",
    "        'num_leaves': [100],\n",
    "        'boosting_type' : ['dart'],\n",
    "        'objective' : ['binary'],\n",
    "        'colsample_bytree' : [0.8],\n",
    "        'reg_alpha' : [0.9],\n",
    "        'reg_lambda' : [0.1],\n",
    "        'subsample_freq': [0.5, 1],\n",
    "        'min_split_gain': [0.01, 0.05], \n",
    "        'min_child_weight': [1, 5],\n",
    "        'scale_pos_weight': [0.5]\n",
    "    }\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "                             objective = 'binary', \n",
    "                             class_weight = 'balanced',\n",
    "                             n_jobs = 1, # Updated from 'nthread' \n",
    "                             silent = False,\n",
    "                             max_depth = -1,\n",
    "                             max_bin = 512,\n",
    "                             subsample_for_bin = 800,\n",
    "                             bagging_freq = 4\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    gs = GridSearchCV(clf, scoring='roc_auc',\n",
    "                      param_grid=gridParams, cv=cv, \n",
    "                      return_train_score=True, n_jobs=-1, verbose = True,\n",
    "                      refit=True)\n",
    "    gs.fit(X_train,Y_train, eval_metric=['roc_auc', 'f1'])\n",
    "    \n",
    "    print ('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print (max(gs.cv_results_['mean_test_score']))\n",
    "    \n",
    "    return gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   54.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0:00:56.189551\n",
      "0.9056704446277791\n",
      "{'boosting_type': 'dart', 'colsample_bytree': 0.8, 'learning_rate': 0.7, 'min_child_weight': 1, 'min_split_gain': 0.05, 'n_estimators': 90, 'num_leaves': 100, 'objective': 'binary', 'reg_alpha': 0.9, 'reg_lambda': 0.1, 'scale_pos_weight': 0.5, 'subsample_freq': 0.5}\n"
     ]
    }
   ],
   "source": [
    "p = Testing_grid_lgb(train, train['Label'].as_matrix())\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec():\n",
    "    cc = CountVectorizer(encoding = 'unicode',\n",
    "                         lowercase = False,\n",
    "                         analyzer = 'char', \n",
    "                         ngram_range = (2,8), \n",
    "                         min_df = 3, \n",
    "                         max_df = 14,\n",
    "                         dtype= np.float32)\n",
    "    cc = cc.fit(train['Word'].as_matrix())\n",
    "    \n",
    "    X_test_cc, X_train_cc = cc.transform(test['Word'].as_matrix()), cc.transform(train['Word'].as_matrix())\n",
    "        \n",
    "    X_train = hstack((X_train_cc, Make_x(train)))\n",
    "    X_test = hstack((X_test_cc, Make_x(test)))\n",
    "    \n",
    "    return X_train, train['Label'].as_matrix(), X_test\n",
    "\n",
    "def Predict_reg(p):\n",
    "    \n",
    "    X_train, y_train, X_test = vec()\n",
    "    clf = LogisticRegression(class_weight = 'balanced', \n",
    "                             solver = 'sag', n_jobs=1,\n",
    "                             C = p['C'],\n",
    "                             penalty = p['penalty'],\n",
    "                             max_iter = p['max_iter'])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def Predict_lgb(p):\n",
    "    X_train, y_train, X_test = vec()\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(boosting_type= 'gbdt', objective = 'binary', \n",
    "                             class_weight = 'balanced',\n",
    "                             n_jobs = 1, # Updated from 'nthread' \n",
    "                             silent = False,\n",
    "                             max_depth = -1,\n",
    "                             max_bin = 512,\n",
    "                             subsample_for_bin = 250,\n",
    "                             bagging_freq = 4,    \n",
    "                             n_estimators = p['n_estimators'],\n",
    "                             learning_rate = p['learning_rate'],\n",
    "                             num_leaves = p['num_leaves'],\n",
    "                             colsample_bytree = p['colsample_bytree'],\n",
    "                             reg_alpha = p['reg_alpha'],\n",
    "                             reg_lambda = p['reg_lambda'],\n",
    "                             subsample_freq = p['subsample_freq'],\n",
    "                             min_split_gain = p['min_split_gain'], \n",
    "                             min_child_weight = p['min_child_weight']\n",
    "                            )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labling trest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:447: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "y_pred = Predict_lgb(p)\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "sample['Prediction'] = y_pred\n",
    "sample.to_csv('res.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
