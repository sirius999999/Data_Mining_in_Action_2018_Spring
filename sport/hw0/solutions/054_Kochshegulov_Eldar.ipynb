{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение соревнования https://www.kaggle.com/c/dmia-surnames-classification\n",
    "##### Нужно построить классификатор, определяющий, является ли слово фамилией. Для оценки качества используется площадь под ROC-кривой (AUC), так как классы сильно несбалансированны.\n",
    "#### Кощегулов Эльдар Public - 0.87451 Private - 0.87753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1ckyro5aololo\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn import cross_validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import xgboost as xgb\n",
    "import pymorphy2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "label_start = train['Label']\n",
    "res = pd.DataFrame(test.index, columns = ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word'] = train['Word'].apply(str.strip)\n",
    "test['Word'] = test['Word'].apply(str.strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерим фичи "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['big'] = train['Word'].apply(lambda x : 1 if list(x)[0].lower() != list(x)[0] else 0)\n",
    "test['big'] = test['Word'].apply(lambda x : 1 if list(x)[0].lower() != list(x)[0] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['upper'] = train['Word'].apply(lambda x : 1 if x.upper() == x else 0)\n",
    "test['upper'] = test['Word'].apply(lambda x : 1 if x.upper() == x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lower'] = train['Word'].apply(lambda x : 1 if x.lower() == x else 0)\n",
    "test['lower'] = test['Word'].apply(lambda x : 1 if x.lower() == x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['apostrophe'] = train['Word'].apply(lambda x : 1 if x.find(\"'\") != -1 else 0)\n",
    "train['hyphen'] = train['Word'].apply(lambda x : 1 if x.find(\"-\") != -1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['apostrophe'] = test['Word'].apply(lambda x : 1 if x.find(\"'\") != -1 else 0)\n",
    "test['hyphen'] = test['Word'].apply(lambda x : 1 if x.find(\"-\") != -1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word'] = train['Word'].apply(str.lower)\n",
    "test['Word'] = test['Word'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Боремся с мусором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word'] = train['Word'].str.replace('á', 'а')\n",
    "train['Word'] = train['Word'].str.replace('é', 'е')\n",
    "train['Word'] = train['Word'].str.replace('ѐ', 'е')\n",
    "train['Word'] = train['Word'].str.replace('ё', 'е')\n",
    "train['Word'] = train['Word'].str.replace('ë', 'е')\n",
    "train['Word'] = train['Word'].str.replace('ó', 'о')\n",
    "train['Word'] = train['Word'].str.replace('ô', 'о')\n",
    "train['Word'] = train['Word'].str.replace('ó', 'о')\n",
    "train['Word'] = train['Word'].str.replace(\"'\", \"'\")\n",
    "train['Word'] = train['Word'].str.replace('’', \"'\")\n",
    "train['Word'] = train['Word'].str.replace('”', \"'\")\n",
    "train['Word'] = train['Word'].str.replace('“', \"'\")\n",
    "train['Word'] = train['Word'].str.replace('`', \"'\")\n",
    "train['Word'] = train['Word'].str.replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Word'] = test['Word'].str.replace('á', 'а')\n",
    "test['Word'] = test['Word'].str.replace('é', 'е')\n",
    "test['Word'] = test['Word'].str.replace('ѐ', 'е')\n",
    "test['Word'] = test['Word'].str.replace('ё', 'е')\n",
    "test['Word'] = test['Word'].str.replace('ë', 'е')\n",
    "test['Word'] = test['Word'].str.replace('ó', 'о')\n",
    "test['Word'] = test['Word'].str.replace('ô', 'о')\n",
    "test['Word'] = test['Word'].str.replace('ó', 'о')\n",
    "test['Word'] = test['Word'].str.replace(\"'\", \"'\")\n",
    "test['Word'] = test['Word'].str.replace('’', \"'\")\n",
    "test['Word'] = test['Word'].str.replace('”', \"'\")\n",
    "test['Word'] = test['Word'].str.replace('“', \"'\")\n",
    "test['Word'] = test['Word'].str.replace('`', \"'\")\n",
    "test['Word'] = test['Word'].str.replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "let_remove = ['.', '/', '2', '3', '4', '7', '«', '·', '»', \"'\", '-', ' ', '&', '0', '1', '\\\\', '\\xa0']\n",
    "for let in let_remove :\n",
    "    train['Word'] = train['Word'].str.replace(let, '')  \n",
    "    test['Word'] = test['Word'].str.replace(let, '')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерим читерские фичи и преобразуем слово к нормальной форме, используя библиотеку pymorphy2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x) :\n",
    "    #print (x)\n",
    "    res = morph.parse(x)\n",
    "    for tmp in res :\n",
    "        #print (tmp.tag)\n",
    "        if set(['Surn', 'sing']) in tmp.tag :\n",
    "            return tmp.inflect({'nomn', 'sing'}).normal_form   \n",
    "    return res[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x) :\n",
    "    #print (x)\n",
    "    res = morph.parse(x)\n",
    "    for tmp in res :\n",
    "        #print (tmp.tag)\n",
    "        if set(['Surn', 'sing']) in tmp.tag :\n",
    "            return 1   \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pymorphy'] = train['Word'].apply(g)\n",
    "test['pymorphy'] = test['Word'].apply(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word'] = train['Word'].apply(f)\n",
    "test['Word'] = test['Word'].apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерим фичи : длина, количество гласных/согласных, оканчивается на буквы 'xy'/'xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['len'] = train['Word'].apply(len)\n",
    "train['glasn'] = train['Word'].apply(lambda x : sum(list(map(x.count, 'аеёиоуэюяы'))))\n",
    "train['soglasn'] = train['len'] - train['glasn']\n",
    "train['2let'] = train['Word'].apply(lambda x : ''.join(list(x)[-2 :]))\n",
    "train['3let'] = train['Word'].apply(lambda x : ''.join(list(x)[-3 :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['len'] = test['Word'].apply(len)\n",
    "test['glasn'] = test['Word'].apply(lambda x : sum(list(map(x.count, 'аеёиоуэюяы'))))\n",
    "test['soglasn'] = test['len'] - test['glasn']\n",
    "test['2let'] = test['Word'].apply(lambda x : ''.join(list(x)[-2 :]))\n",
    "test['3let'] = test['Word'].apply(lambda x : ''.join(list(x)[-3 :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2 = (train['2let'].value_counts() > train['2let'].value_counts().median()).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col3 = (train['3let'].value_counts() > train['3let'].value_counts().median()).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for let2 in col2[: 50] :\n",
    "    train['last' + '_' + let2] = train['2let'].apply(lambda x : 1 if x == let2 else 0)   \n",
    "    test['last' + '_' + let2] = test['2let'].apply(lambda x : 1 if x == let2 else 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for let3 in col3[: 50] :\n",
    "    train['last' + '_' + let3] = train['3let'].apply(lambda x : 1 if x == let3 else 0)   \n",
    "    test['last' + '_' + let3] = test['3let'].apply(lambda x : 1 if x == let3 else 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерим фичи частоты окончаний вида 'xy'/'xyz'\n",
    "<font color = 'red'>Эти 2 фичи добавил уже после окончания соревнования(дают +0.02 к auc)<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['2let_freq'] = train['2let'].map(train['2let'].value_counts() / train.shape[0])\n",
    "train['3let_freq'] = train['3let'].map(train['3let'].value_counts() / train.shape[0])\n",
    "test['2let_freq'] = test['2let'].map(train['2let'].value_counts() / train.shape[0])\n",
    "test['3let_freq'] = test['3let'].map(train['3let'].value_counts() / train.shape[0])\n",
    "test['2let_freq'] = test['2let_freq'].fillna(0)\n",
    "test['3let_freq'] = test['3let_freq'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Все это грузим в xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['Label']\n",
    "train = train.drop(['Word', 'Label', '2let', '3let'], axis = 1)\n",
    "test = test.drop(['Word', '2let', '3let'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binding to float32\n",
      "Building DMatrix...\n",
      "Training ...\n",
      "[0]\ttrain-auc:0.883556\tvalid-auc:0.877586\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 100 rounds.\n",
      "[10]\ttrain-auc:0.90401\tvalid-auc:0.891968\n",
      "[20]\ttrain-auc:0.909948\tvalid-auc:0.894179\n",
      "[30]\ttrain-auc:0.914063\tvalid-auc:0.895756\n",
      "[40]\ttrain-auc:0.916522\tvalid-auc:0.896794\n",
      "[50]\ttrain-auc:0.918742\tvalid-auc:0.897595\n",
      "[60]\ttrain-auc:0.921013\tvalid-auc:0.898273\n",
      "[70]\ttrain-auc:0.922426\tvalid-auc:0.898949\n",
      "[80]\ttrain-auc:0.923833\tvalid-auc:0.898549\n",
      "[90]\ttrain-auc:0.925493\tvalid-auc:0.899082\n",
      "[100]\ttrain-auc:0.926483\tvalid-auc:0.899353\n",
      "[110]\ttrain-auc:0.927669\tvalid-auc:0.89956\n",
      "[120]\ttrain-auc:0.928731\tvalid-auc:0.899487\n",
      "[130]\ttrain-auc:0.929902\tvalid-auc:0.899881\n",
      "[140]\ttrain-auc:0.930911\tvalid-auc:0.899805\n",
      "[150]\ttrain-auc:0.931953\tvalid-auc:0.900086\n",
      "[160]\ttrain-auc:0.93271\tvalid-auc:0.900054\n",
      "[170]\ttrain-auc:0.933681\tvalid-auc:0.900195\n",
      "[180]\ttrain-auc:0.934572\tvalid-auc:0.900339\n",
      "[190]\ttrain-auc:0.935378\tvalid-auc:0.900329\n",
      "[200]\ttrain-auc:0.936057\tvalid-auc:0.900403\n",
      "[210]\ttrain-auc:0.936604\tvalid-auc:0.900655\n",
      "[220]\ttrain-auc:0.937219\tvalid-auc:0.900753\n",
      "[230]\ttrain-auc:0.937787\tvalid-auc:0.900735\n",
      "[240]\ttrain-auc:0.93834\tvalid-auc:0.900901\n",
      "[250]\ttrain-auc:0.938864\tvalid-auc:0.900829\n",
      "[260]\ttrain-auc:0.939434\tvalid-auc:0.900963\n",
      "[270]\ttrain-auc:0.93991\tvalid-auc:0.901013\n",
      "[280]\ttrain-auc:0.940252\tvalid-auc:0.900912\n",
      "[290]\ttrain-auc:0.940843\tvalid-auc:0.900828\n",
      "[300]\ttrain-auc:0.941239\tvalid-auc:0.90072\n",
      "[310]\ttrain-auc:0.941686\tvalid-auc:0.900696\n",
      "[320]\ttrain-auc:0.941993\tvalid-auc:0.900542\n",
      "[330]\ttrain-auc:0.942585\tvalid-auc:0.900589\n",
      "[340]\ttrain-auc:0.942985\tvalid-auc:0.900724\n",
      "Stopping. Best iteration:\n",
      "[244]\ttrain-auc:0.938544\tvalid-auc:0.901117\n",
      "\n",
      "AUC XgBoost:  0.932317762636695\n"
     ]
    }
   ],
   "source": [
    "x_train,  x_valid, y_train, y_valid = train_test_split(train, label, random_state = 456)\n",
    "\n",
    "print('Building DMatrix...')\n",
    "d_train = xgb.DMatrix(x_train, label = y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label = y_valid)\n",
    "\n",
    "print('Training ...')\n",
    "\n",
    "params = {'max_depth': 10, \n",
    "          'min_child_weight': 50, \n",
    "          'lambda': 0.1,\n",
    "          'alpha': 0.1,\n",
    "          'eta': 0.5, \n",
    "          'objective': 'binary:logistic',\n",
    "          'eval_metric' : 'auc',\n",
    "          'silent': 1,\n",
    "          'seed': 95\n",
    "         }\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "model1 = xgb.train(params, d_train, 30000, watchlist, early_stopping_rounds = 100, verbose_eval = 10)\n",
    "d_train_full = xgb.DMatrix(train, label)\n",
    "print('AUC XgBoost: ', roc_auc_score(label_start, model1.predict(d_train_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(test)\n",
    "res['Prediction'] = model1.predict(d_test)\n",
    "res.to_csv('data/res.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
