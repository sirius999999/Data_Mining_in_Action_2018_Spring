{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Спорт 2018 весна] hw0\n",
    "## Nikita Fomin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import log_loss, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer, normalize, StandardScaler\n",
    "\n",
    "from natasha import NamesExtractor\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_columns = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "real_test = pd.read_csv('test.csv')['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать дубликаты с разными лейблами\n",
    "\n",
    "train['WordLower'] = train['Word'].apply(str.lower)\n",
    "\n",
    "train['Duplicate'] = train.duplicated(subset='Word', keep=False)\n",
    "train['DuplicateLowerWord'] = train.duplicated(subset='WordLower', keep=False)\n",
    "train['DuplicateLowerWordLabel'] = train.duplicated(subset=['WordLower', 'Label'], keep=False)\n",
    "\n",
    "train = train[(train['DuplicateLowerWord'] == False) | (((train['DuplicateLowerWord'] == True)) & ((train['DuplicateLowerWordLabel'] == True)))]\n",
    "train.drop(['WordLower', 'Duplicate', 'DuplicateLowerWord', 'DuplicateLowerWordLabel'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделать отложенную выборку\n",
    "\n",
    "# np.random.seed(890)\n",
    "# msk = np.random.rand(len(train)) < 0.333\n",
    "\n",
    "# deferred_test = train[~msk]\n",
    "# y_deferred_true = deferred_test['Label']\n",
    "# X_deferred = deferred_test.drop('Label', axis=1)\n",
    "# train = train[msk]\n",
    "\n",
    "# print(len(deferred_test))\n",
    "# del deferred_test\n",
    "# y_deferred_true.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Искуственно уравняем классы\n",
    "\n",
    "majority = train[train['Label'] == 0]\n",
    "minority = train[train['Label'] == 1]\n",
    " \n",
    "minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=348) \n",
    "train = pd.concat([majority, minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Для извлечения фич из слов\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, words):\n",
    "        \n",
    "        def pymorphy_check(x):\n",
    "            \"\"\"Является ли именем по версии pymorphy\"\"\"\n",
    "            return int('Name' in morph.parse(x)[0].tag)\n",
    "    \n",
    "        def natasha_check(x):\n",
    "            \"\"\"Является ли именем по версии natasha\"\"\"\n",
    "            return int(bool(extractor(x)))\n",
    "        \n",
    "        def check_upper(x):\n",
    "            \"\"\"Есть ли заглавные буквы среди символов строки\"\"\"\n",
    "            return any(map(str.isupper, x))\n",
    "        \n",
    "        def is_noun(x):\n",
    "            \"\"\"Является ли существительным\"\"\"\n",
    "            return int('NOUN' in morph.parse(x)[0].tag)\n",
    "    \n",
    "        extractor = NamesExtractor()\n",
    "        morph = MorphAnalyzer()\n",
    "        \n",
    "        typical_endings = (\"ев\", \"ов\", \"ских\", \"ко\",\"заде\", \"ли\", \"лы\", \"оглу\", \n",
    "                           \"кызы\", \"ян\", \"янц\", \"уни\", \"ич\", \"ов\", \"ук\", \"ик\", \"ски\", \n",
    "                            \"ка\", \"ини\", \"ук\", \"юк\", \"ун\", \"ний\", \"ный\", \"чай\", \"ий\", \"а\", \n",
    "                            \"ишин\", \"ску\", \"ул\", \"ан\", \"цки\", \"ман\", \"ер\", \"те\", \"ис\", \"не\", \"пулос\", \n",
    "                            \"кос\", \"иди\", \"швили\", \"дзе\", \"ури\", \n",
    "                            \"иа\", \"уа\", \"ава\", \"ли\", \"си\", \"ни\", \"огло\")\n",
    "        \n",
    "        \n",
    "        df = words.to_frame(name='word')\n",
    "        df['word_lower'] = df['word'].apply(str.lower)\n",
    "        \n",
    "        df['length'] = df['word'].apply(len)\n",
    "        df['is_letter'] = df['word'].apply(lambda x: 1 if len(x) == 1 else 0)\n",
    "        \n",
    "        df['is_name_pymorpy'] = df['word'].apply(pymorphy_check)\n",
    "        df['is_name_natasha'] = df['word'].apply(natasha_check)\n",
    "        df['is_noun'] = df['word'].apply(is_noun)\n",
    "        \n",
    "        df['typical_ending'] = df['word_lower'].apply(lambda x: int(x.lower().endswith(typical_endings)))\n",
    "        \n",
    "        df['cnt_vowels'] = df['word_lower'].apply(lambda x: len(re.findall('[аоэиуыеёюя]', x)))\n",
    "        df['cnt_consonants'] = df['word_lower'].apply(lambda x: len(re.findall('[бвгджзйклмнпрстфхцчшщ]', x)))\n",
    "        \n",
    "        df['signs'] = df['word_lower'].apply(lambda x: len(re.findall('[ьъ]', x)))\n",
    "        \n",
    "        df['digits'] = df['word'].apply(lambda x: len(re.findall('[0-9]', x)))\n",
    "        df['symbols'] = df['word'].apply(lambda x: len(x) - len(re.sub(r'[^\\w\\s]', '', x)))\n",
    "        \n",
    "        df['dot'] = df['word'].apply(lambda x: 1 if x.find('.') != -1 else 0)\n",
    "        df['apostrophe'] = df['word'].apply(lambda x: 1 if x.find('`') != -1 or x.find('\\'') != -1 else 0)\n",
    "        \n",
    "        df['is_upper'] = df['word'].apply(lambda x: int(x.isupper()))\n",
    "        df['is_lower'] = df['word'].apply(lambda x: int(x.islower()))\n",
    "        \n",
    "        df['is_first_upper'] = df['word'].apply(lambda x: 1 if x[0].isupper() and x[1:].islower() else 0)\n",
    "        df['is_first_lower'] = df['word'].apply(lambda x: 1 if x[0].islower() and check_upper(x[1:]) else 0)\n",
    "        \n",
    "        df.drop(['word', 'word_lower'], axis=1, inplace=True)\n",
    "        df = df.to_dict('records')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            ('n_grams', Pipeline([\n",
    "                ('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 7))),\n",
    "                ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "            ])),\n",
    "\n",
    "            ('features', Pipeline([\n",
    "                ('features_selector', WordFeatures()),\n",
    "                ('vect', DictVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        \n",
    "        transformer_weights={\n",
    "            'n_grams': 1.0,\n",
    "            'features': 1.0,\n",
    "        },\n",
    "    )),\n",
    "    \n",
    "    ('scaler', Normalizer(norm='l2')),\n",
    "    ('clf', MultinomialNB()),\n",
    "#     ('clf', RandomForestClassifier(n_estimators=50, random_state=197, n_jobs=-1, max_features='auto')),\n",
    "#     ('clf', XGBClassifier(n_estimators=150, max_depth=15, seed=10, objective='binary:logistic', learning_rate=0.09 , colsample_bytree=0.9 , colsample_bylevel=0.6)),\n",
    "#     ('clf', LogisticRegression(C=200, n_jobs=-1, solver='saga', penalty='l1', class_weight={0: 1, 1: 10})),\n",
    "#     ('clf', SVC(C=1.0))\n",
    "#     ('clf', SGDClassifier(loss='log', penalty='elasticnet', alpha=1e-05, class_weight={0: 1, 1: 10}, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Label']\n",
    "X = train['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'clf__max_depth': [9, 11], \n",
    "#     'clf__min_child_weight': [45, 50, 55],\n",
    "#     'clf__subsample': [0.97, 0.99, 1],\n",
    "#     'clf__n_estimators': [20, 30],\n",
    "#     'clf__colsample_bytree': [0.95, 0.97, 1]\n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#     'clf__loss': ['log'],\n",
    "#     'clf__penalty': ['elasticnet'],\n",
    "#     'clf__alpha': [1e-05],\n",
    "#     'clf__l1_ratio': [0.15],\n",
    "#     'clf__class_weight': [{0: 1, 1: 10}]\n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#     'clf__C': [1, 100, 1000],\n",
    "#     'clf__penalty': ['l1', 'l2'],\n",
    "#     'clf__solver': ['saga'],\n",
    "#     'clf__class_weight': [{0: 1, 1: 10}],\n",
    "#     'clf__n_jobs': [-1]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, params, n_jobs=-1, verbose=1, scoring='roc_auc')\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cv = ShuffleSplit(n_splits=5, test_size=0.666, random_state=1159)\n",
    "# scores = cross_val_score(pipeline, X, y, n_jobs=-1, scoring='roc_auc', cv=cv)\n",
    "# print(\"ROC AUC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('n_grams', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0,...malizer(copy=True, norm='l2')), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pipeline.predict_proba(real_test)[:,1]\n",
    "\n",
    "submission = pd.DataFrame(submission, columns=['Prediction']).reset_index()\n",
    "submission.columns = ['Id', 'Prediction']\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y, pipeline.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_deferred = X_deferred['Word']\n",
    "# y_deferred = pipeline.predict_proba(X_deferred)[:,1]\n",
    "\n",
    "# roc_auc_score(y_deferred_true, y_deferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
