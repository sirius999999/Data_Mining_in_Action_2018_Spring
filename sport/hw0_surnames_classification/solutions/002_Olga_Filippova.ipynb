{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Основная идея решения заключается в том, что последовательность данных является очень важным признаком. Те, если внимательно посмотреть на данные, можно обнаружить много закономерностей, для слов, идущих по порядку. Получается, что для получения максимального скора необходимо построить модель, которая не просто хорошо определяет, является ли слово \"Иванов\" фамилией, а которое определяет это, при условии, что перед этим словом было слово \"ИВАНОВ\", а после еще 5 похожих на него слов. \n",
    "\n",
    "Кому лень смотреть код (а он абсолютно не стоит смотрения), основные фичи: написание (с большой буквы, с маленькой, капсом), то же самое для предыдущего слова, является ли предыдущее слово дублем, является ли предыдущее слово \"однокоренным\" (начинается ли текущее слово с предыдущего без трех букв), агрегация сколько в группе подряд идущих дублей, подряд идущих похожих слов, плюс OHE на окончания (последние две буквы). на второй неделе добавились признаки из pymorphy - падеж, часть речи, множественное число для текущего слова и для предыдущего, natasha и pymystem3, потыренные у Кирилла, признаки про второе предыдущее слово. (как позже выяснилось, последние два не привели к улучшению скора на приватном лб)\n",
    "\n",
    "Все это отправлялось в XGBClassifier, валидировалась на отложенной выборке, данные делила по букве, чтобы не нарушать последоватьельность похожих слов.  \n",
    "\n",
    "в конце постобработка - для слов, которые являются дублями учитывая регистр, первому присваивается 0, второму 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#читаем данные\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#добавляем признак - слово написанное строчными буквами\n",
    "train['word']=train['Word'].apply(lambda x: x.lower())\n",
    "test['word']=test['Word'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#признак написание слова: капсом, с заглавной буквы, с маленькой буквы\n",
    "def is_starts(x):\n",
    "    if x.isupper(): r=2\n",
    "    elif x[0].isupper(): r=1\n",
    "    elif x[0].islower(): r=0\n",
    "    else: r=100\n",
    "    return r\n",
    "\n",
    "train['is_starts']=train['Word'].map(is_starts)\n",
    "test['is_starts']=test['Word'].map(is_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Признаки связанные с предыдущим словом. Какое написание, является ли дублем, \n",
    "#является ли \"однокоренным\" словом (текущее слово начинается с предыдущего слова без трех последних букв )\n",
    "\n",
    "train['nb_is_starts']=np.append(np.array([0]), train['is_starts'][:-1].values)\n",
    "\n",
    "nb=train.word[0]\n",
    "is_dbl=[]\n",
    "is_syn=[]\n",
    "for i in train.word[1:]:\n",
    "    if nb==i:\n",
    "        is_dbl.append(1)\n",
    "        is_syn.append(1)\n",
    "    else:\n",
    "        is_dbl.append(0)\n",
    "        if (len(nb)>3 and i.startswith(nb[:-3])) or (len(nb)==3 and i.startswith(nb[:-2]) or i.startswith(nb)):\n",
    "            is_syn.append(1)\n",
    "        else: is_syn.append(0)\n",
    "    nb=i\n",
    "train['is_dbl']=np.append(np.array([0]), np.array(is_dbl))\n",
    "train['is_syn']=np.append(np.array([0]), np.array(is_syn))\n",
    "\n",
    "#test\n",
    "\n",
    "test['nb_is_starts']=np.append(np.array([0]), test['is_starts'][:-1].values)\n",
    "\n",
    "nb=test.word[0]\n",
    "is_dbl=[]\n",
    "is_syn=[]\n",
    "for i in test.word[1:]:\n",
    "    if nb==i:\n",
    "        is_dbl.append(1)\n",
    "        is_syn.append(1)\n",
    "    else:\n",
    "        is_dbl.append(0)\n",
    "        if (len(nb)>3 and i.startswith(nb[:-3])) or (len(nb)==3 and i.startswith(nb[:-2]) or i.startswith(nb)):\n",
    "            is_syn.append(1)\n",
    "        else: is_syn.append(0)\n",
    "    nb=i\n",
    "test['is_dbl']=np.append(np.array([0]), np.array(is_dbl))\n",
    "test['is_syn']=np.append(np.array([0]), np.array(is_syn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#теперь добавим аггригированные значения. сколько синонимов в группе, сколько дублей у слова.\n",
    "l=[]\n",
    "n=0\n",
    "for i in train.is_syn:\n",
    "    if i==0: \n",
    "        l=np.append(l, [n]*n+[0])\n",
    "        n=0\n",
    "    else:\n",
    "        n+=1\n",
    "\n",
    "train['agg_syn']=np.append(l, [n]*n).astype(int)\n",
    "\n",
    "train['agg_dbl']=train.groupby('word')['Word'].transform('count')\n",
    "\n",
    "#test\n",
    "l=[]\n",
    "n=0\n",
    "for i in test.is_syn:\n",
    "    if i==0: \n",
    "        l=np.append(l, [n]*n+[0])\n",
    "        n=0\n",
    "    else:\n",
    "        n+=1\n",
    "       \n",
    "test['agg_syn']=np.append(l, [n]*n).astype(int)\n",
    "\n",
    "test['agg_dbl']=test.groupby('word')['Word'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#признаки состоит ли слово только из букв и начинается ли с \"о'\"\n",
    "train['is_a']=train['Word'].apply(lambda x: x.isalpha()).astype(int)\n",
    "train['apst']=train['word'].apply(lambda x: 1 if x.startswith(\"о'\") else 0).astype(int)\n",
    "\n",
    "\n",
    "test['is_a']=test['Word'].apply(lambda x: x.isalpha()).astype(int)\n",
    "test['apst']=test['word'].apply(lambda x: 1 if x.startswith(\"о'\") else 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#работа с окончаниями. добавляет в трейн и тест столбец с окончаниями. для окончаний, которые в трейне представлены \n",
    "#малым количеством слов (<20), создается окончание 'idx0'. оно же присваивается окончаниям, которые не встречались в train\n",
    "\n",
    "def prepr(X, Z):\n",
    "    X['ends']=X['word'].apply(lambda x: x[-2:])\n",
    "    Z['ends']=Z['word'].apply(lambda x: x[-2:])\n",
    "    \n",
    "    temp=X.groupby('ends')['Label'].agg([len])\n",
    "    idx0=temp[temp.len<20].index\n",
    "    \n",
    "    def temp1(x):\n",
    "        if x in idx0: k='idx0'\n",
    "        else: k=x\n",
    "        return k\n",
    "    \n",
    "    a=X['ends'].map(temp1)\n",
    "    le=LabelEncoder()\n",
    "    le.fit(a)\n",
    "    X['ends']=le.transform(a)\n",
    "   \n",
    "    def temp2(x):\n",
    "        if x in temp[temp.len>=20].index: k=x\n",
    "        else: k='idx0'\n",
    "        return k\n",
    "    \n",
    "    b=Z['ends'].map(temp2)\n",
    "    Z['ends']=le.transform(b)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Признаки из pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#часть речи, падеж, число, начальная форма, слова, идущие подряд с одинаковой начальной формой\n",
    "train['pos']=train['word'].apply(lambda x: morph.parse(x)[0].tag.POS)\n",
    "train['case']=train['word'].apply(lambda x: morph.parse(x)[0].tag.case)\n",
    "train['number']=train['word'].apply(lambda x: morph.parse(x)[0].tag.number)\n",
    "train['norm']=train['word'].apply(lambda x: morph.parse(x)[0].normal_form)\n",
    "train['agg_norm']=train.groupby('norm')['Word'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['pos']=test['word'].apply(lambda x: morph.parse(x)[0].tag.POS)\n",
    "test['case']=test['word'].apply(lambda x: morph.parse(x)[0].tag.case)\n",
    "test['number']=test['word'].apply(lambda x: morph.parse(x)[0].tag.number)\n",
    "test['norm']=test['word'].apply(lambda x: morph.parse(x)[0].normal_form)\n",
    "test['agg_norm']=test.groupby('norm')['Word'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#часть речи, падеж, число для соседей сверху\n",
    "train['nb_pos']=np.append(np.array(['fff']), train['pos'][:-1].values)\n",
    "train['nb_case']=np.append(np.array(['fff']), train['case'][:-1].values)\n",
    "train['nb_number']=np.append(np.array(['fff']), train['number'][:-1].values)\n",
    "\n",
    "test['nb_pos']=np.append(np.array(['fff']), test['pos'][:-1].values)\n",
    "test['nb_case']=np.append(np.array(['fff']), test['case'][:-1].values)\n",
    "test['nb_number']=np.append(np.array(['fff']), test['number'][:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#применим ко всему этому LabelEncoder\n",
    "le=LabelEncoder()\n",
    "\n",
    "cols_for_le=['pos','case','number']\n",
    "ads_cols_for_le=['nb_pos','nb_case','nb_number']\n",
    "les=[]\n",
    "for i in cols_for_le:\n",
    "    les.append(LabelEncoder().fit(train[i].append(test[i]).fillna('fff').astype(str)))\n",
    "    \n",
    "for n in range(len(cols_for_le)):\n",
    "    train[cols_for_le[n]]=les[n].transform(train[cols_for_le[n]].fillna('fff').astype(str))\n",
    "    train[ads_cols_for_le[n]]=les[n].transform(train[ads_cols_for_le[n]].fillna('fff').astype(str))\n",
    "    test[cols_for_le[n]]=les[n].transform(test[cols_for_le[n]].fillna('fff').astype(str))\n",
    "    test[ads_cols_for_le[n]]=les[n].transform(test[ads_cols_for_le[n]].fillna('fff').astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#добавим наташу \n",
    "from natasha import NamesExtractor\n",
    "func = NamesExtractor()\n",
    "def function_natasha(word):\n",
    "    return 1 if func(word) else 0\n",
    "\n",
    "n_train=train['Word'].apply(function_natasha)\n",
    "n_test=test['Word'].apply(function_natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#добавим pymystem3\n",
    "import pymystem3\n",
    "mystem = pymystem3.Mystem()\n",
    "\n",
    "def name_from_pymystem(word):\n",
    "    try:\n",
    "        return 1 if 'имя' in mystem.analyze(word)[0]['analysis'][0]['gr'].split(',') else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def surn_from_pymystem(word):\n",
    "    try:\n",
    "        return 1 if 'фам' in mystem.analyze(word)[0]['analysis'][0]['gr'].split(',') else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "train_nm= train['Word'].apply(name_from_pymystem)\n",
    "train_sur = train['Word'].apply(surn_from_pymystem)\n",
    "test_nm= test['Word'].apply(name_from_pymystem)\n",
    "test_sur = test['Word'].apply(surn_from_pymystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n",
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "# и еще пожалуй добавим данные про еще одного соседа сверху\n",
    "a=np.hstack([np.append(np.array([0,0]), train['is_starts'][:-2].values).reshape(-1, 1),\n",
    "            np.append(np.array([0]), train['nb_pos'][:-1].values).reshape(-1, 1),\n",
    "            np.append(np.array([0]), train['nb_case'][:-1].values).reshape(-1, 1),\n",
    "            np.append(np.array([0]), train['nb_number'][:-1].values).reshape(-1, 1),\n",
    "             n_train.reshape(-1, 1), np.append(np.array([0]), n_train[:-1].values).reshape(-1, 1),\n",
    "            train_nm.values.reshape(-1, 1),train_sur.values.reshape(-1, 1),\n",
    "            np.append(np.array([0]), train_nm[:-1].values).reshape(-1, 1),\n",
    "            np.append(np.array([0]), train_sur[:-1].values).reshape(-1, 1)])\n",
    "\n",
    "\n",
    "b=np.hstack([np.append(np.array([0,0]), test['is_starts'][:-2].values).reshape(-1, 1),\n",
    "   np.append(np.array([0]), test['nb_pos'][:-1].values).reshape(-1, 1),\n",
    "   np.append(np.array([0]), test['nb_case'][:-1].values).reshape(-1, 1),\n",
    "   np.append(np.array([0]), test['nb_number'][:-1].values).reshape(-1, 1),\n",
    "            n_test.reshape(-1, 1), np.append(np.array([0]), n_test[:-1].values).reshape(-1, 1),\n",
    "            test_nm.reshape(-1, 1),test_sur.reshape(-1, 1),\n",
    "            np.append(np.array([0]), test_nm[:-1].values).reshape(-1, 1),\n",
    "            np.append(np.array([0]), test_sur[:-1].values).reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n",
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#Формируем файл с ответами\n",
    "\n",
    "prepr(train, test)\n",
    "\n",
    "xgb=XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=500)\n",
    "ohe=OneHotEncoder()\n",
    "ohe.fit(train.iloc[:,19].reshape(-1, 1))\n",
    "trn=hstack((train[train.columns[3:14].append(train.columns[15:-1])],ohe.transform(train.iloc[:,19].reshape(-1, 1)),a))\n",
    "tsn=hstack((test[train.columns[3:14].append(train.columns[15:-1])],ohe.transform(test.iloc[:,18].reshape(-1, 1)),b))\n",
    "xgb.fit(trn, train.Label)\n",
    "y=xgb.predict_proba(tsn)[:,1]\n",
    "#pd.DataFrame(y,columns=['Prediction']).to_csv('subm_2_1', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#кривая постобработка в 2 часа ночи) \n",
    "tmp=test[['Word', 'apst']]\n",
    "tmp['f_dbl']=tmp.groupby('Word')['apst'].transform('count')\n",
    "tmp['y']=y.Prediction\n",
    "tmp['res']=[999]*tmp.shape[0]\n",
    "tmp.loc[tmp.apst==1, 'res']=[1]*sum(tmp.apst==1)\n",
    "for w in tmp[tmp.f_dbl==2]['Word'].unique(): \n",
    "    l=np.append(l, tmp[tmp.Word==w].index.values)\n",
    "tmp.loc[l, 'res']=[0,1]*len(rrr)\n",
    "tmp['res1']=tmp.apply(lambda x: x[3] if x[4]==999 else x[4], axis=1 )\n",
    "pd.DataFrame(tmp.res1.values,columns=['Prediction']).to_csv('subm_2_3', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
